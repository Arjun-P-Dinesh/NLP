{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MAI372â€“ Natural Language Processing<h3>\n",
    "III MSc AIM<br>\n",
    "19-03-2024<br>\n",
    "Regular lab Question\n",
    "<h6>Submitted by Arjun P Dinesh (2348506)</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/arj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/arj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/arj/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/arj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading lancaster_stemmer: Package\n",
      "[nltk_data]     'lancaster_stemmer' not found in index\n",
      "[nltk_data] Error loading porter_stemmer: Package 'porter_stemmer' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Error loading snowball_stemmer: Package 'snowball_stemmer'\n",
      "[nltk_data]     not found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('lancaster_stemmer')\n",
    "nltk.download('porter_stemmer')\n",
    "nltk.download('snowball_stemmer')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Write a program to get Antonyms from WordNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonyms(word):\n",
    "    # Get the synsets for the given word\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    # Initialize an empty list to store the antonyms\n",
    "    antonyms = []\n",
    "    \n",
    "    # Iterate over each synset and get its lemmas\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            # Check if the lemma has any antonyms\n",
    "            if len(lemma.antonyms()) > 0:\n",
    "                # Add the antonym to the list\n",
    "                antonyms.append(lemma.antonyms()[0].name())\n",
    "                \n",
    "    return antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take input from the user\n",
    "word = input(\"Enter a word: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonyms for 'pack': ['unpack']\n"
     ]
    }
   ],
   "source": [
    "# Print out the antonyms of the entered word\n",
    "if len(get_antonyms(word)) == 0:\n",
    "    print(f\"No antonyms found for '{word}'\")\n",
    "else:\n",
    "    print(f\"Antonyms for '{word}': {get_antonyms(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Write a program for stemming non-English words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping language codes to SnowballStemmer classes\n",
    "STEMMERS = {\n",
    "    'de': SnowballStemmer('german'),\n",
    "    'es': SnowballStemmer('spanish'),\n",
    "    'fr': SnowballStemmer('french'),\n",
    "    'it': SnowballStemmer('italian'),\n",
    "    'en': SnowballStemmer('english')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user to enter a word and its language code\n",
    "word = input(\"Enter a word: \")\n",
    "language_code = input('''Enter its language code  \\n (en for english, de for German, es for Spanish,fr for french, it for italian ): ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the language code is valid\n",
    "if language_code not in STEMMERS:\n",
    "    raise ValueError(f\"Invalid language code '{language_code}', supported languages are {' '.join(sorted(STEMMERS.keys()))}.\")\n",
    "\n",
    "# Tokenize the word into individual characters\n",
    "tokens = word_tokenize(word)\n",
    "\n",
    "# Create a stemmer object for the specified language\n",
    "stemmer = STEMMERS[language_code]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stemmed word for 'running' is 'run'\n"
     ]
    }
   ],
   "source": [
    "# Apply the stemmer to the tokens and join them back together\n",
    "stemmed_word = ''.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "# Print out the stemmed word\n",
    "print(f\"The stemmed word for '{word}' is '{stemmed_word}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Write a program for lemmatizing words Using WordNet (Use all type of stemmers for the comparison)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to lemmatize a word using WordNetLemmatizer\n",
    "def lemmatize_with_pos(word, pos=None):\n",
    "    \"\"\"Lemmatize a word using WordNetLemmatizer.\"\"\"\n",
    "    if pos is None:\n",
    "        pos = 'v'  # Default verb POS tag\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        return lemmatizer.lemmatize(word, pos=pos)\n",
    "    except Exception as e:\n",
    "        print(f\"Error lemmatizing '{word}' with POS tag '{pos}': {str(e)}\")\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user to enter a word\n",
    "word = input(\"Enter a word to lemmitize: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WordNet Lemmatizer (default verb POS tag): run\n",
      "WordNet Lemmatizer (noun POS tag): running\n",
      "WordNet Lemmatizer (adj POS tag): running\n",
      "WordNet Lemmatizer (adv POS tag): running\n",
      "\n",
      "PorterStemmer: run\n",
      "\n",
      "LancasterStemmer: run\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nWordNet Lemmatizer (default verb POS tag):\", lemmatize_with_pos(word))\n",
    "stop_words = set(stopwords.words('english'))\n",
    "if word not in stop_words:\n",
    "    print(\"WordNet Lemmatizer (noun POS tag):\", lemmatize_with_pos(word, pos='n'))\n",
    "    print(\"WordNet Lemmatizer (adj POS tag):\", lemmatize_with_pos(word, pos='a'))\n",
    "    print(\"WordNet Lemmatizer (adv POS tag):\", lemmatize_with_pos(word, pos='r'))\n",
    "\n",
    "# Perform stemming with PorterStemmer\n",
    "print(\"\\nPorterStemmer:\", PorterStemmer().stem(word))\n",
    "\n",
    "# Perform stemming with LancasterStemmer\n",
    "print(\"\\nLancasterStemmer:\", LancasterStemmer().stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Write a program to differentiate stemming and lemmatizing words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to lemmatize a list of words using WordNetLemmatizer\n",
    "def lemmatize_list(words):\n",
    "    \"\"\"Lemmatize a list of words using WordNetLemmatizer.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Define a function to stem a list of words using PorterStemmer\n",
    "def porter_stem_list(words):\n",
    "    \"\"\"Stem a list of words using PorterStemmer.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Define a function to stem a list of words using LancasterStemmer\n",
    "def lancaster_stem_list(words):\n",
    "    \"\"\"Stem a list of words using LancasterStemmer.\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Words:\n",
      "  ['running', 'running', 'run', 'man']\n",
      "\n",
      "WordNet Lemmatizer:\n",
      "  ['running', 'running', 'run', 'man']\n",
      "\n",
      "PorterStemmer:\n",
      "  ['run', 'run', 'run', 'man']\n",
      "\n",
      "LancasterStemmer:\n",
      "  ['run', 'run', 'run', 'man']\n",
      "\n",
      "Frequency Distribution of Original Words:\n",
      "Counter({'running': 2, 'run': 1, 'man': 1})\n",
      "\n",
      "Frequency Distribution of WordNet Lemmatizer:\n",
      "Counter({'running': 2, 'run': 1, 'man': 1})\n",
      "\n",
      "Frequency Distribution of PorterStemmer:\n",
      "Counter({'run': 3, 'man': 1})\n",
      "\n",
      "Frequency Distribution of LancasterStemmer:\n",
      "Counter({'run': 3, 'man': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prompt the user to enter a list of words separated by spaces\n",
    "words = input(\"Enter a list of words separated by spaces: \").split()\n",
    "\n",
    "# Remove punctuation marks and convert to lowercase\n",
    "words = [word.strip(string.punctuation).lower() for word in words]\n",
    "\n",
    "# Filter out stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Display the original list of words\n",
    "print(\"\\nOriginal Words:\")\n",
    "print(\" \", words)\n",
    "\n",
    "# Perform lemmatization with WordNetLemmatizer\n",
    "print(\"\\nWordNet Lemmatizer:\")\n",
    "lemmatized_words = lemmatize_list(words)\n",
    "print(\" \", lemmatized_words)\n",
    "\n",
    "# Perform stemming with PorterStemmer\n",
    "print(\"\\nPorterStemmer:\")\n",
    "stemmed_words = porter_stem_list(words)\n",
    "print(\" \", stemmed_words)\n",
    "\n",
    "# Perform stemming with LancasterStemmer\n",
    "print(\"\\nLancasterStemmer:\")\n",
    "lanstemmed_words = lancaster_stem_list(words)\n",
    "print(\" \", lanstemmed_words)\n",
    "\n",
    "# Count the frequency distribution of each unique word in each list\n",
    "freq_dist_original = Counter(words)\n",
    "freq_dist_lemmatized = Counter(lemmatized_words)\n",
    "freq_dist_porters = Counter(stemmed_words)\n",
    "freq_dist_lancers = Counter(lanstemmed_words)\n",
    "\n",
    "# Display the frequency distributions\n",
    "print(\"\\nFrequency Distribution of Original Words:\")\n",
    "print(freq_dist_original)\n",
    "\n",
    "print(\"\\nFrequency Distribution of WordNet Lemmatizer:\")\n",
    "print(freq_dist_lemmatized)\n",
    "\n",
    "print(\"\\nFrequency Distribution of PorterStemmer:\")\n",
    "print(freq_dist_porters)\n",
    "\n",
    "print(\"\\nFrequency Distribution of LancasterStemmer:\")\n",
    "print(freq_dist_lancers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Write a program for PoS Tagging and also execute any of the tool that given in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/arj/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package treebank to /home/arj/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform PoS tagging on a single sentence\n",
    "def pos_tag_sentence(sentence):\n",
    "    \"\"\"Perform PoS tagging on a single sentence.\"\"\"\n",
    "    tagged_sentence = nltk.pos_tag(TreebankWordTokenizer().tokenize(sentence))\n",
    "    return tagged_sentence\n",
    "\n",
    "# Read a sentence from the user\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "# Perform PoS tagging on the sentence\n",
    "tagged_sentence = pos_tag_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tagged Sentence:\n",
      "1. Arjun       NNP\n",
      "2. P           NNP\n",
      "3. Dinesh      NNP\n",
      "4. said        VBD\n",
      "5. The         DT\n",
      "6. high-performanceNN\n",
      "7. GT2         NNP\n",
      "8. version     NN\n",
      "9. made        VBD\n",
      "10. a           DT\n",
      "11. return      NN\n",
      "12. to          TO\n",
      "13. the         DT\n",
      "14. 991         CD\n",
      "15. lineage     NN\n",
      "16. but         CC\n",
      "17. now         RB\n",
      "18. as          IN\n",
      "19. an          DT\n",
      "20. RS          NNP\n",
      "21. variant     NN\n",
      "22. only        RB\n",
      "23. with        IN\n",
      "24. no          DT\n",
      "25. standard    NN\n",
      "26. variant     NN\n",
      "27. being       VBG\n",
      "28. produced    VBN\n",
      "29. ,           ,\n",
      "30. unlike      IN\n",
      "31. the         DT\n",
      "32. previous    JJ\n",
      "33. generations.NN\n",
      "34. It          PRP\n",
      "35. was         VBD\n",
      "36. initially   RB\n",
      "37. unveiled    VBN\n",
      "38. at          IN\n",
      "39. the         DT\n",
      "40. 2017        CD\n",
      "41. E3          NNP\n",
      "42. along       IN\n",
      "43. with        IN\n",
      "44. the         DT\n",
      "45. announcementNN\n",
      "46. of          IN\n",
      "47. the         DT\n",
      "48. Forza       NNP\n",
      "49. Motorsport  NNP\n",
      "50. 7           CD\n",
      "51. .           .\n"
     ]
    }
   ],
   "source": [
    "# Display the tagged sentence\n",
    "print(\"\\nTagged Sentence:\")\n",
    "for i, (word, tag) in enumerate(tagged_sentence):\n",
    "    print(f\"{i+1}. {word:<12}{tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Write a program to identify the Named Entity Recognition and also execute any of the tool that given in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform named entity recognition on a single sentence\n",
    "def ner_tag_sentence(sentence):\n",
    "    \"\"\"Perform named entity recognition on a single sentence.\"\"\"\n",
    "    ne_tags = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)))\n",
    "    return ne_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a paragraph from the user\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/arj/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/arj/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package names to /home/arj/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities from each sentence\n",
    "named_entities = []\n",
    "for sentence in sentences:\n",
    "    ne_tags = ner_tag_sentence(sentence)\n",
    "    for ne_tag in ne_tags:\n",
    "        if hasattr(ne_tag, 'label'):\n",
    "            named_entity = (' '.join([word for word, pos in ne_tag]), ne_tag.label())\n",
    "            named_entities.append(named_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Named Entities:\n",
      "1. ARJUN (ORGANIZATION)\n",
      "2. GT2 (ORGANIZATION)\n",
      "3. Forza (ORGANIZATION)\n"
     ]
    }
   ],
   "source": [
    "# Display the extracted named entities\n",
    "print(\"\\nExtracted Named Entities:\")\n",
    "for i, (entity, label) in enumerate(named_entities):\n",
    "    print(f\"{i+1}. {entity} ({label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Implement the Dependency Parsing and Constituency Parsing using the tool**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home/arj/TriSem3/NLP/Lab/Lab5/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constituency Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home/arj/TriSem3/NLP/Lab/Lab5/c_parsing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"/home/arj/TriSem3/NLP/Lab/Lab5/d_parse.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
